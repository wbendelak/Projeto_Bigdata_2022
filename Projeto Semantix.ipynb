{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto final Semantix - Jupyter Notebook ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utizando o Docker, foi realizado download do arquivo do link informado abaixo:\n",
    "https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "Os comandos usados foram:\n",
    "wget https://mobileapps.saude.gov.br/esus-vepi/files/\n",
    "unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "Após o download do arquivo, foi realizado a descompressão através do comando:\n",
    "unrar e 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar /home/wbendelak/treinamentos/spark/input/dados_covid/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **1. Enviar os dados para o hdfs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transferir dados o HDFS\n",
    "\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv /user/william/projeto_final/dados_covid\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv /user/william/projeto_final/dados_covid\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /user/william/projeto_final/dados_covid\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /user/william/projeto_final/dados_covid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação dos dados no HDFS\n",
    "!hdfs dfs -ls /user/william/projeto_final/dados_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regiao;estado;municipio;coduf;codmun;codRegiaoSaude;nomeRegiaoSaude;data;semanaEpi;populacaoTCU2019;casosAcumulado;casosNovos;obitosAcumulado;obitosNovos;Recuperadosnovos;emAcompanhamentoNovos;interior/metropolitana\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-25;9;210147125;0;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-26;9;210147125;1;1;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-27;9;210147125;1;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-28;9;210147125;1;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-29;9;210147125;2;1;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-03-01;10;210147125;2;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-03-02;10;210147125;2;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-03-03;10;210147125;2;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-03-04;10;210147125;3;1;0;0;;;\r",
      "\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "#Análise do conteúdo e estrutura dos arquivos\n",
    "\n",
    "!hdfs dfs -cat /user/william/projeto_final/dados_covid/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados .csv para o DataFrame dados_covid_csv e posteriormente salvando os dados no HDFS no formato Parquet com compressão SNAPPY\n",
    "dados_covid_csv = spark.read.csv(\"/user/william/projeto_final/dados_covid\",sep=\";\",header=\"True\",inferSchema=\"True\")\n",
    "dados_covid_csv.write.parquet(\"/user/william/projeto_final/tabelas/dadoscovid.parquet.snappy.partitioned.by.municipio\",partitionBy=\"municipio\",compression=\"SNAPPY\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2624943"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisar o schema dos campos do DataFrame para usar como referência na criação da tabela hive.\n",
    "#Contar quantidade de registros para comparar posteriormente com a tabela hive.\n",
    "dados_covid_csv.printSchema()\n",
    "dados_covid_csv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| databaseName|\n",
      "+-------------+\n",
      "|      default|\n",
      "|projeto_final|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criação da Base de Dados a ser usada no Projeto caso o mesmo não exista\n",
    "\n",
    "spark.sql(\"create database if not exists projeto_final\")\n",
    "spark.sql(\"show databases\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para este projeto, a tabela não será incrementada, por isso se faz necessário a exclusão da tabela caso a mesma já exista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS dados_covid_parquet_snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação da tabela externa através do próprio spark.sql otimizando a mesma no formato parquet, compressão SNAPPY e particionando por município\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE dados_covid_parquet_snappy(regiao string,\n",
    "                         estado string,\n",
    "                         cod_uf int,\n",
    "                         codmun int,\n",
    "                         codRegiaoSaude int,\n",
    "                         nomeRegiaoSaude string,\n",
    "                         data timestamp,\n",
    "                         semanaEpi int,\n",
    "                         populacaoTCU2019 int,\n",
    "                         casosAcumulado decimal(10,0),\n",
    "                         casosNovos int,\n",
    "                         obitosAcumulado int,\n",
    "                         obitosNovos int,\n",
    "                         Recuperadosnovos int,\n",
    "                         emAcompanhamentoNovos int,\n",
    "                         interior_metropolitana string)\n",
    "                         stored as parquet\n",
    "                         partitioned by (municipio string)\n",
    "                         location 'hdfs://namenode:8020/user/william/projeto_final/tabelas/dadoscovid.parquet.snappy.partitioned.by.municipio'            \n",
    "                         tblproperties(\"skip.header.line.count\"=\"1\",'parquet.compress'='SNAPPY')\n",
    "                         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visando minimizar riscos de falhas devido manipulação das partições via HDFS por exemplo, é realizado a reparação da tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"msck repair table dados_covid_parquet_snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validação da tabela criada e comparando se a quantidade de registros está igual ao dos arquivos originais em CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+\n",
      "|     database|           tableName|isTemporary|\n",
      "+-------------+--------------------+-----------+\n",
      "|projeto_final|   casos_confirmados|      false|\n",
      "|projeto_final|   casos_recuperados|      false|\n",
      "|projeto_final|dados_covid_parqu...|      false|\n",
      "|projeto_final|  obitos_confirmados|      false|\n",
      "|projeto_final|visualizacao_caso...|      false|\n",
      "+-------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use projeto_final\")\n",
    "spark.sql(\"show tables\").show()\n",
    "spark.sql(\"select count(*) from dados_covid_parquet_snappy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Criar as 3 vizualizações(descritas no pdf do projeto) pelo Spark com os dados enviados para o HDFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- cod_uf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior_metropolitana: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#atribui a table dados covid a variavel tabela\n",
    "tabela = spark.read.table(\"dados_covid_parquet_snappy\")\n",
    "tabela.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Casos_Recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|Em_Acompanhamento|\n",
      "+-----------------+\n",
      "|          1065477|\n",
      "+-----------------+\n",
      "\n",
      "+---------+\n",
      "|Acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n",
      "+-----------+\n",
      "|Casos_novos|\n",
      "+-----------+\n",
      "|      62504|\n",
      "+-----------+\n",
      "\n",
      "+----------+\n",
      "|incidencia|\n",
      "+----------+\n",
      "|    8972.3|\n",
      "+----------+\n",
      "\n",
      "+---------------+\n",
      "|Obito_acumulado|\n",
      "+---------------+\n",
      "|         526892|\n",
      "+---------------+\n",
      "\n",
      "+------------+\n",
      "|Obitos_novos|\n",
      "+------------+\n",
      "|        1780|\n",
      "+------------+\n",
      "\n",
      "+----------+\n",
      "|letalidade|\n",
      "+----------+\n",
      "|       2.8|\n",
      "+----------+\n",
      "\n",
      "+-----------+\n",
      "|mortalidade|\n",
      "+-----------+\n",
      "|      250.7|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Preparação das querys para obtenção das informações.\n",
    "#Na apresentação dos resultados, está sendo usado o filtro por data igual a 06/07/2021.\n",
    "#As fórmulas de Incidência, Letalidade e Mortalidade foram obtidas do próprio site https://covid.saude.gov.br/ na aba \"sobre\".\n",
    "\n",
    "#Para o total de casos recuperados, optou-se por buscar o maior registro da coluna Recuperadosnovos.\n",
    "total_casos_recuperados = spark.sql(\"select MAX (Recuperadosnovos) as Casos_Recuperados from dados_covid_parquet_snappy\").show()\n",
    "\n",
    "#Para o total de casos em acompanhamento, optou-se por buscar o último registro inserido na tabela, excluindo-se os valores nulos.\n",
    "casos_em_acompanhamento = spark.sql(\"select LAST (emAcompanhamentoNovos) as Em_Acompanhamento from dados_covid_parquet_snappy WHERE emAcompanhamentoNovos IS NOT NULL\").show()\n",
    "\n",
    "#Para o total de casos acumulados, optou-se por buscar o maior registro da coluna casosAcumulado.\n",
    "total_casos_acumulados = spark.sql(\"select MAX (casosAcumulado) as Acumulado from dados_covid_parquet_snappy \").show()\n",
    "\n",
    "#Para o total de casos novos, optou-se por buscar o maior registro da coluna casosNovos.\n",
    "casos_novos = spark.sql(\"select MAX (casosNovos) as Casos_novos from dados_covid_parquet_snappy where data = ('2021-07-06')\").show()\n",
    "\n",
    "#Incidêcia = (Número de casos confirmados de COVID-19 em residentes X 100.000) / População total residente no período determinado.\n",
    "incidencia = spark.sql(\"SELECT ROUND(((MAX(casosAcumulado) / MAX(populacaoTCU2019))*100000),1) as incidencia from dados_covid_parquet_snappy where data = ('2021-07-06')\").show()\n",
    "\n",
    "#Para o total de óbitos acumulados, optou-se por buscar o maior registro da coluna obitosAcumulado.\n",
    "total_obitos_acumulados = spark.sql(\"select MAX (obitosAcumulado) as Obito_acumulado from dados_covid_parquet_snappy \").show()\n",
    "\n",
    "#Para o total de óbitos novos, optou-se por buscar o maior registro da coluna obitosNovos.\n",
    "obitos_novos = spark.sql(\"select MAX (obitosNovos) as Obitos_novos from dados_covid_parquet_snappy where data = ('2021-07-06')\").show()\n",
    "\n",
    "#Letalidade = (Número de óbitos confirmados de COVID-19 em determinada área e período X 100) / Número de casos confirmados de COVID-19 em determinada área e período.\n",
    "letalidade = spark.sql(\"SELECT ROUND(((MAX(obitosAcumulado) / MAX(casosAcumulado))*100),1) as letalidade from dados_covid_parquet_snappy\").show()\n",
    "\n",
    "#Mortalidade = (Número de óbitos confirmados de COVID-19 em residentes X 100.000) / População total residente no período determinado.\n",
    "mortalidade = spark.sql(\"SELECT ROUND(((MAX(obitosAcumulado) / MAX(populacaoTCU2019))*100000),1) as mortalidade from dados_covid_parquet_snappy\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criação de 3 views de apresentação solicitada no projeto\n",
    "\n",
    "#View 1: Casos recuperados + Casos em Acompanhamento\n",
    "total_casos_recuperados = spark.sql(\"CREATE OR REPLACE VIEW casos_Recuperados AS select 'Casos_Recuperados', MAX (Recuperadosnovos) as Total_Casos_Recuperados from dados_covid_parquet_snappy UNION select 'Em_acompanhamento', MAX (emAcompanhamentoNovos) as Em_Acompanhamento from dados_covid_parquet_snappy WHERE data = ('2021-07-06')\").show()\n",
    "\n",
    "#View 2: Casos Confirmados Acumulado + Casos Confirmados Novos + Incidência de Casos Confirmados\n",
    "casos_confirmados = spark.sql(\"CREATE OR REPLACE VIEW casos_confirmados AS select 'Casos_Recuperados', (MAX (casosAcumulado)) as Total_Casos_Confirmados from dados_covid_parquet_snappy UNION select 'Casos_novos', MAX (casosNovos) as Casos_novos from dados_covid_parquet_snappy where data = ('2021-07-06') UNION SELECT 'Incidencia', ROUND(((MAX(casosAcumulado) / MAX(populacaotcu2019))*100000),1) as incidencia from dados_covid_parquet_snappy where data = ('2021-07-06')\").show()\n",
    "\n",
    "#View 3: Óbitos Acumulado + Novos Óbitos + Letalidade + Mortalidade\n",
    "Obitos_confirmados = spark.sql(\"CREATE OR REPLACE VIEW obitos_confirmados AS select 'Obitos_Confirmados', MAX (obitosAcumulado) as Total_Obitos_Confirmados from dados_covid_parquet_snappy UNION select 'obitos_novos', MAX (obitosNovos) as Obitos_novos from dados_covid_parquet_snappy where data = ('2021-07-06') UNION SELECT 'obitos_acumulado', ROUND(((MAX(obitosAcumulado) / MAX(casosAcumulado))*100),1) as letalidade from dados_covid_parquet_snappy UNION SELECT 'mortalidade', ROUND(((MAX(obitosAcumulado) / MAX(populacaoTCU2019))*100000),1) as mortalidade from dados_covid_parquet_snappy\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n",
      "|Casos_Recuperados|total_casos_recuperados|\n",
      "+-----------------+-----------------------+\n",
      "|Em_acompanhamento|                1065477|\n",
      "|Casos_Recuperados|               17262646|\n",
      "+-----------------+-----------------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "| Acumulados|casos_confirmados|\n",
      "+-----------+-----------------+\n",
      "|Casos_novos|          62504.0|\n",
      "| Incidencia|           8972.3|\n",
      "| Acumulados|       18855015.0|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------------+------------------+\n",
      "|    Total_obitos|Obitos_confirmados|\n",
      "+----------------+------------------+\n",
      "|    Total_obitos|          526892.0|\n",
      "|    obitos_novos|            1780.0|\n",
      "|obitos_acumulado|               2.8|\n",
      "|     mortalidade|             250.7|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Executar as 3 view para validar o resultado\n",
    "\n",
    "spark.sql(\"SELECT * from casos_Recuperados\").show()\n",
    "spark.sql(\"SELECT * from casos_confirmados\").show()\n",
    "spark.sql(\"SELECT * from obitos_confirmados\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Salvar a primeira visualização como tabela Hive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-7cf121c0e701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#visualizacao_casos_recuperados = spark.sql(\"SELECT * from casos_recuperados\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#visualizacao_casos_recuperados.write.saveAsTable(\"visualizacao_casos_recuperados\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvisualizacao_casos_recuperados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Criar um DataFrame para receber os dados da primeira View casos_Recuperados, e salvar em tabela hive\n",
    "\n",
    "visualizacao_casos_recuperados = spark.sql(\"SELECT * from casos_recuperados\")\n",
    "visualizacao_casos_recuperados.write.saveAsTable(\"visualizacao_casos_recuperados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n",
      "|Casos_Recuperados|Total_Casos_Recuperados|\n",
      "+-----------------+-----------------------+\n",
      "|Em_acompanhamento|                1065477|\n",
      "|Casos_Recuperados|               17262646|\n",
      "+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validar tabela a tabela visualizacao_casos_recuperados\n",
    "\n",
    "spark.sql(\"SELECT * from visualizacao_casos_recuperados\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Salvar a segunda visualização com formato parquet e compressão snappy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame para receber os dados da segunda View casos_confirmados, e salvar no formato parquet com compressão SNAPPY\n",
    "\n",
    "visualizacao_casos_confirmados = spark.sql(\"SELECT * from casos_confirmados\")\n",
    "visualizacao_casos_confirmados.write.option(\"compression\",\"snappy\").parquet('hdfs://namenode:8020/user/william/projeto_final/tabelas/visualizacao_casos_confirmados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n",
      "|Casos_Recuperados|Total_Casos_Confirmados|\n",
      "+-----------------+-----------------------+\n",
      "|Casos_Recuperados|             18855015.0|\n",
      "|      Casos_novos|                62504.0|\n",
      "|       Incidencia|                 8972.3|\n",
      "+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validar a leitura dos dados\n",
    "\n",
    "spark.read.parquet(\"hdfs://namenode:8020/user/william/projeto_final/tabelas/visualizacao_casos_confirmados\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-05-01 02:51 /user/william/projeto_final/tabelas/visualizacao_casos_confirmados/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        427 2022-05-01 02:51 /user/william/projeto_final/tabelas/visualizacao_casos_confirmados/part-00000-bdb21380-7e83-4f13-8211-15837cd13883-c000.snappy.parquet\r\n",
      "-rw-r--r--   2 root supergroup        851 2022-05-01 02:51 /user/william/projeto_final/tabelas/visualizacao_casos_confirmados/part-00132-bdb21380-7e83-4f13-8211-15837cd13883-c000.snappy.parquet\r\n",
      "-rw-r--r--   2 root supergroup        905 2022-05-01 02:51 /user/william/projeto_final/tabelas/visualizacao_casos_confirmados/part-00146-bdb21380-7e83-4f13-8211-15837cd13883-c000.snappy.parquet\r\n",
      "-rw-r--r--   2 root supergroup        842 2022-05-01 02:51 /user/william/projeto_final/tabelas/visualizacao_casos_confirmados/part-00151-bdb21380-7e83-4f13-8211-15837cd13883-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# Validar o diretório criado para a segunda view casos_confirmados\n",
    "\n",
    "!hdfs dfs -ls /user/william/projeto_final/tabelas/visualizacao_casos_confirmados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Salvar a terceira visualização em um tópico no Kafka.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Usando o terminal kafka, foi criado o tópico topic-projeto-final, e habilitado o consumer no mesmo através dos comandos abaixo:\n",
    "\n",
    "#kafka-topics.sh --bootstrap-server kafka:9092 --topic topic-projeto-final --create --partitions 1 --replication-factor 1\n",
    "#kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic topic-projeto-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviar os dados para o tópico topic-projeto-final\n",
    "\n",
    "#importar as functions e types  necessárias\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Criação de dataframe para receber os dados da view obitos_confirmados\n",
    "obitos_confirmados_view = spark.sql(\"SELECT * from obitos_confirmados\")\n",
    "\n",
    "#Criação de dataframe para receber os valores das colunas Obitos_Confirmados e Total_Obitos_Confirmados\n",
    "obitos_confirmados_kafka = obitos_confirmados_view.withColumn(\"value\", struct(col(\"Obitos_Confirmados\"),col(\"Total_Obitos_Confirmados\")))\n",
    "\n",
    "#Transformação do dataframe para remover o cabeçalho das colunas Obitos_Confirmados e Total_Obitos_Confirmados\n",
    "obitos_confirmados_kafka = obitos_confirmados_kafka.drop(\"Obitos_Confirmados\",\"Total_Obitos_Confirmados\")\n",
    "\n",
    "#Transformação da coluna value do dataframe para String \n",
    "obitos_confirmados_kafka = obitos_confirmados_kafka.withColumn(\"value\",col(\"value\").cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[obitos_novos, 17...|\n",
      "|[Obitos_Confirmad...|\n",
      "|[obitos_acumulado...|\n",
      "|[mortalidade, 250.7]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validação dos dados do dataframe\n",
    "obitos_confirmados_kafka.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Envio dos dados para o tópico topic-projeto-final\n",
    "obitos_confirmados_kafka.write.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:9092\").option(\"topic\",\"topic-projeto-final\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Criar a visualização pelo Spark com os dados enviados para o HDFS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Será utilizado os dados salvos em parquet com compressão SNAPPY no HDFS /user/william/projeto_final/tabelas/dadoscovid.parquet.snappy.partitioned.by.municipio\n",
    "dados_covid_parquet = spark.read.parquet(\"hdfs://namenode:8020/user/william/projeto_final/tabelas/dadoscovid.parquet.snappy.partitioned.by.municipio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Síntese de casos, óbitos, incidência e mortalidade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regiao</th>\n",
       "      <th>Casos</th>\n",
       "      <th>Óbitos</th>\n",
       "      <th>Incidência/100mil hab</th>\n",
       "      <th>Mortalidade /100mil hab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brasil</td>\n",
       "      <td>18855015</td>\n",
       "      <td>526892</td>\n",
       "      <td>8972.2926259</td>\n",
       "      <td>250.725295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Centro-Oeste</td>\n",
       "      <td>1916619</td>\n",
       "      <td>49207</td>\n",
       "      <td>11760.5098928</td>\n",
       "      <td>301.937636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nordeste</td>\n",
       "      <td>4455737</td>\n",
       "      <td>107824</td>\n",
       "      <td>7807.2680354</td>\n",
       "      <td>188.927414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norte</td>\n",
       "      <td>1732862</td>\n",
       "      <td>43845</td>\n",
       "      <td>9401.8983255</td>\n",
       "      <td>237.887513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sudeste</td>\n",
       "      <td>7138803</td>\n",
       "      <td>245311</td>\n",
       "      <td>8078.1795176</td>\n",
       "      <td>277.590836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sul</td>\n",
       "      <td>3611041</td>\n",
       "      <td>80705</td>\n",
       "      <td>12046.4469156</td>\n",
       "      <td>269.232196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         regiao     Casos  Óbitos Incidência/100mil hab  \\\n",
       "0        Brasil  18855015  526892          8972.2926259   \n",
       "1  Centro-Oeste   1916619   49207         11760.5098928   \n",
       "2      Nordeste   4455737  107824          7807.2680354   \n",
       "3         Norte   1732862   43845          9401.8983255   \n",
       "4       Sudeste   7138803  245311          8078.1795176   \n",
       "5           Sul   3611041   80705         12046.4469156   \n",
       "\n",
       "   Mortalidade /100mil hab  \n",
       "0               250.725295  \n",
       "1               301.937636  \n",
       "2               188.927414  \n",
       "3               237.887513  \n",
       "4               277.590836  \n",
       "5               269.232196  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importar funções de col e lit para utilizar nos agrupamentos\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Realizado os agrupamentos e filtros\n",
    "sintese_covid = dados_covid_parquet.select('regiao','populacaoTCU2019','obitosAcumulado','casosAcumulado','estado','data').where(dados_covid_parquet.regiao != 'regiao')\n",
    "\n",
    "sintese_covid = sintese_covid.groupBy('estado', 'regiao')\n",
    "sintese_covid = sintese_covid.max('casosAcumulado','obitosAcumulado','populacaoTCU2019')\n",
    "\n",
    "sintese_covid = sintese_covid.groupBy('regiao')\n",
    "sintese_covid = sintese_covid.sum('max(casosAcumulado)','max(obitosAcumulado)','max(populacaoTCU2019)')\n",
    "sintese_covid = sintese_covid.withColumn('cem_mil', lit(100000))\n",
    "\n",
    "sintese_covid = sintese_covid.withColumn('Incidência/100mil hab', ((col('sum(max(casosAcumulado))') * col('cem_mil'))/col('sum(max(populacaoTCU2019))')))\n",
    "sintese_covid = sintese_covid.withColumn('Mortalidade /100mil hab', ((col('sum(max(obitosAcumulado))')/col('sum(max(populacaoTCU2019))'))) * col('cem_mil'))\n",
    "\n",
    "\n",
    "sintese_covid = sintese_covid.drop('cem_mil','sum(max(populacaoTCU2019))')\n",
    "sintese_covid = sintese_covid.withColumnRenamed('sum(max(casosAcumulado))', 'Casos').withColumnRenamed('sum(max(obitosAcumulado))', 'Óbitos')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sintese_covid = sintese_covid.sort(\"regiao\")\n",
    "\n",
    "cabecalho_sintese_covid = \"Síntese de casos, óbitos, incidência e mortalidade\"\n",
    "print(cabecalho_sintese_covid)\n",
    "#usado o toPandas para usar na memória\n",
    "sintese_covid.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Salvar a visualização do exercício 6 em um tópico no Elastic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação de dataframe para receber os dados da view obitos_confirmados\n",
    "obitos_confirmados_elastic = spark.sql(\"SELECT * from obitos_confirmados\")\n",
    "\n",
    "#Salvar dados como csv para posterior exportação para o elastic\n",
    "obitos_confirmados_elastic.write.format('csv').option(\"inferSchema\", \"true\").option(\"header\",\"true\").save(\"hdfs://namenode:8020/user/william/projeto_final/elastic_view/obitos_confirmados.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obitos_Confirmados</th>\n",
       "      <th>Total_Obitos_Confirmados</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Obitos_Confirmados</td>\n",
       "      <td>526892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obitos_acumulado</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obitos_novos</td>\n",
       "      <td>1780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mortalidade</td>\n",
       "      <td>250.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Obitos_Confirmados  Total_Obitos_Confirmados\n",
       "0  Obitos_Confirmados                  526892.0\n",
       "1    obitos_acumulado                       2.8\n",
       "2        obitos_novos                    1780.0\n",
       "3         mortalidade                     250.7"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validação do csv criado\n",
    "spark.read.csv('hdfs://namenode:8020/user/william/projeto_final/elastic_view/obitos_confirmados.csv', inferSchema = True, header = True).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
